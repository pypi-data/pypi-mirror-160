{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0d4109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a46aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import llamass.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d14399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spl\n",
    "from llamass.core import unpack_body_models, npz_paths\n",
    "import tempfile\n",
    "import warnings\n",
    "import gaitplotlib.core as gpl\n",
    "body_model = gpl.init_body_model(\"neutral\")\n",
    "fk_engine = transforms.SMPLHForwardKinematics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d402b4fb",
   "metadata": {},
   "source": [
    "# Losses\n",
    "\n",
    "> A selection of loss functions I've used with this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85969a0c",
   "metadata": {},
   "source": [
    "## Axis-Angle Cosine Distance\n",
    "\n",
    "> Cosine similarity between rotation vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2f86cd",
   "metadata": {},
   "source": [
    "This is probably not a correct distance metric for rotations. [This blog][belousov] makes an argument that there's only one correct distance metric for rotations and it's the angular distance between unit quaternions. But, this one is quick to evaluate on rotation vectors.\n",
    "\n",
    "There are also implementations of this correct distance metric using rotation matrices in Python [here][ahrs].\n",
    "\n",
    "[ahrs]: https://github.com/Mayitzin/ahrs/blob/master/ahrs/utils/metrics.py\n",
    "[belousov]: http://www.boris-belousov.net/2016/12/01/quat-dist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e6156c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "def aa_cosine(out, target):\n",
    "    if out.ndim == 2:\n",
    "        b, d = out.size()\n",
    "    elif out.ndim == 3:\n",
    "        b, f, d = out.size()\n",
    "        assert f == 1, f'{out.size()}'\n",
    "    j = d//3\n",
    "    out, target = out.view(b, j, 3), target.view(b, j, 3)\n",
    "    def theta(x, eps=1e-6):\n",
    "        return torch.sqrt(torch.clamp(torch.sum(x**2, 2, keepdims=True), eps, 2*math.pi))\n",
    "    theta_a = theta(out)\n",
    "    theta_b = theta(target)\n",
    "    cosine_sim = F.cosine_similarity(out, target, dim=2)\n",
    "    cosine_sim_loss = 1. - cosine_sim\n",
    "    cosine_angle_diff = 1. - torch.cos(theta_a - theta_b)\n",
    "    return cosine_sim_loss + cosine_angle_diff[:,:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbf2cc0",
   "metadata": {},
   "source": [
    "## VPoser\n",
    "\n",
    "> The loss function used by the VPoser VAE in the [SMPL-X][smplx] paper.\n",
    "\n",
    "Appears to cause NaNs when used on the sample AMASS data below in tests.\n",
    "\n",
    "[smplx]: https://ps.is.tuebingen.mpg.de/uploads_file/attachment/attachment/497/SMPL-X.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afd6124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "class GeodesicLossR(nn.Module):\n",
    "    def __init__(self, reduction='batchmean'):\n",
    "        super(GeodesicLossR, self).__init__()\n",
    "\n",
    "        self.reduction = reduction\n",
    "        self.eps = 1e-6\n",
    "\n",
    "    # batch geodesic loss for rotation matrices\n",
    "    def bgdR(self,m1,m2):\n",
    "        assert m1.ndim == m2.ndim, \\\n",
    "            f\"Rotation matrices ndim must be equal but was {m1.ndim} {m2.ndim}\"\n",
    "        for m in [m1, m2]:\n",
    "            assert m.size(-1) == 3 and m.size(-2) == 3, \\\n",
    "                f\"Trailing 2 dimensions must 3x3 rotation matrices {m.size()}\"\n",
    "        if m1.ndim == 2:\n",
    "            # ndim 2 must be single rotation matrix\n",
    "            m1 = m1.view(1, 3, 3)\n",
    "            m2 = m2.view(1, 3, 3)\n",
    "        elif m1.ndim > 3:\n",
    "            m1 = m1.view(-1, 3, 3)\n",
    "            m2 = m2.view(-1, 3, 3)\n",
    "        batch = m1.shape[0]\n",
    "        m = torch.bmm(m1, m2.transpose(1, 2))  # batch*3*3\n",
    "\n",
    "        cos = (m[:, 0, 0] + m[:, 1, 1] + m[:, 2, 2] - 1) / 2\n",
    "        cos = torch.min(cos, m1.new(np.ones(batch)))\n",
    "        cos = torch.max(cos, m1.new(np.ones(batch)) * -1)\n",
    "\n",
    "        return torch.acos(cos)\n",
    "\n",
    "    def forward(self, ypred, ytrue):\n",
    "        theta = self.bgdR(ypred,ytrue)\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(theta)\n",
    "        elif self.reduction == 'batchmean':\n",
    "            return torch.mean(torch.sum(theta, dim=theta.shape[1:]))\n",
    "        elif self.reduction == 'none':\n",
    "            return theta\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Reduction {self.reduction} not known\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d86b20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "class ContinuousRotReprDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, module_input):\n",
    "        b, d = module_input.size()\n",
    "        assert d%6 == 0\n",
    "        reshaped_input = module_input.view(-1, 3, 2)\n",
    "\n",
    "        b1 = F.normalize(reshaped_input[:, :, 0], dim=1)\n",
    "\n",
    "        dot_prod = torch.sum(b1 * reshaped_input[:, :, 1], dim=1, keepdim=True)\n",
    "        b2 = F.normalize(reshaped_input[:, :, 1] - dot_prod * b1, dim=-1)\n",
    "        b3 = torch.cross(b1, b2, dim=1)\n",
    "\n",
    "        return torch.stack([b1, b2, b3], dim=-1).view(b, -1, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf9d205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "class ForwardKinematicLoss(nn.Module):\n",
    "    \"Must be initialized with an SMPL-like `body_model`.\"\n",
    "    def __init__(self, body_model):\n",
    "        super().__init__()\n",
    "        self.bm = body_model\n",
    "        self.geodesic_loss = GeodesicLossR(reduction=\"mean\")\n",
    "        \n",
    "    def kinematics(self, aa_out, pose_target):\n",
    "        with torch.no_grad():\n",
    "            bm_orig = self.bm(pose_body=pose_target)\n",
    "        bm_rec = self.bm(pose_body=aa_out.contiguous())\n",
    "        return bm_orig, bm_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a3bc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "class VPoserLikelihood(ForwardKinematicLoss):\n",
    "    def forward(\n",
    "        self,\n",
    "        dec_out,\n",
    "        aa_out,\n",
    "        pose_target,\n",
    "        pose_target_rotmat,\n",
    "        bm_orig=None,\n",
    "        bm_rec=None,\n",
    "        loss_rec_wt=torch.tensor(4),\n",
    "        loss_matrot_wt=torch.tensor(2),\n",
    "        loss_jtr_wt=torch.tensor(2),\n",
    "        callback=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Default settings for loss weights taken from:\n",
    "        https://github.com/nghorbani/human_body_prior/blob/master/src/human_body_prior/train/V02_05/V02_05.yaml\n",
    "        Inputs:\n",
    "            - dec_out: output of network as rotation matrix, shape (batch, frames, joints, 3, 3)\n",
    "            - aa_out: output of network as axis-angle vectors, shape (batch, frames, joints, 3)\n",
    "            - pose_target: target as axis-angle vectors, shape (batch, frames, joints, 3)\n",
    "            - pose_target_rotmat: target as rotation matrix, shape (batch, frames, joints, 3, 3)\n",
    "        \"\"\"\n",
    "        l1_loss = torch.nn.L1Loss(reduction=\"mean\")\n",
    "\n",
    "        # cast decoder output to aa\n",
    "        bs, f, d = pose_target.size()\n",
    "        \n",
    "        # forward kinematics\n",
    "        if bm_orig is None or bm_rec is None:\n",
    "            bm_orig, bm_rec = self.kinematics(aa_out.view(bs*f, -1), pose_target.view(bs*f, d))\n",
    "\n",
    "        # Reconstruction loss - L1 on the output mesh\n",
    "        v2v = l1_loss(bm_rec.v, bm_orig.v)\n",
    "\n",
    "        # Geodesic loss between rotation matrices\n",
    "        matrot_loss = self.geodesic_loss(\n",
    "            dec_out.view(-1, 3, 3),\n",
    "            pose_target_rotmat.view(-1, 3, 3)\n",
    "        )\n",
    "        # L1 Loss on joint positions\n",
    "        jtr_loss = l1_loss(bm_rec.Jtr, bm_orig.Jtr)\n",
    "\n",
    "        # apply weights to make weighted loss\n",
    "        weighted_loss = (\n",
    "            loss_matrot_wt * matrot_loss + loss_rec_wt * v2v + loss_jtr_wt * jtr_loss\n",
    "        )\n",
    "\n",
    "\n",
    "        # log results\n",
    "        with torch.no_grad():\n",
    "            unweighted_loss = matrot_loss + v2v + jtr_loss\n",
    "            if callback is not None:\n",
    "                callback(all_univariate_tensors_in(locals()))\n",
    "\n",
    "        return weighted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a73eef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b65de4abae2e496ab62896da14e30151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor(False)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spl\n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    unpack_body_models(\"sample_data/\", tmpdirname, 1, verify=False)\n",
    "    for npz_path in npz_paths(tmpdirname):\n",
    "        cdata = np.load(npz_path)\n",
    "        _poses = torch.tensor(cdata['poses'][::6, 3:66], dtype=torch.float, requires_grad=True)\n",
    "        n, d = _poses.size()\n",
    "        j = d//3\n",
    "        poses = _poses.reshape(n, 1, j*3)\n",
    "        vposer_loss = VPoserLikelihood(body_model)\n",
    "        pred, target = poses[:-1], poses[1:]\n",
    "        n = pred.size(0)\n",
    "        pred_rotmat = transforms.Rotation.from_rotvec(pred.reshape(-1, 3)).as_matrix().view(n, -1)\n",
    "        target_rotmat = transforms.Rotation.from_rotvec(target.reshape(-1, 3)).as_matrix().view(n, -1)\n",
    "        loss = vposer_loss(pred_rotmat, pred, target, target_rotmat)\n",
    "        # loss = poses.sum()\n",
    "        loss.backward()\n",
    "        try:\n",
    "            assert not torch.any(torch.isnan(_poses.grad)), \"gradient contains NaNs\"\n",
    "            assert torch.abs(_poses.grad).max() > 1e-6, \"gradient is zero\"\n",
    "        except AssertionError as e:\n",
    "            warnings.warn(str(e))\n",
    "        break\n",
    "torch.any(torch.isnan(_poses.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba193b99",
   "metadata": {},
   "source": [
    "## Discretized Euler Angles\n",
    "\n",
    "> Intuitively, an ADC applied to Euler angles. Allows use of a NLL loss, which is can be useful for training a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88d42ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def discretize(x, nquant, eps=1e-6, dither=False, zero_centered=True):\n",
    "    if zero_centered:\n",
    "        x = x + math.pi\n",
    "    m = math.pi*2\n",
    "    assert x.max() < m\n",
    "    x = x/m # scale to between zero and 1\n",
    "    x = x*nquant\n",
    "    if dither:\n",
    "        d = 2.*(torch.rand_like(x)-0.5)\n",
    "        x = torch.clamp(x+d, 0, nquant-eps)\n",
    "    return torch.floor(x).long() # bin account to nquant levels\n",
    "\n",
    "class DiscretizedEulerLoss(nn.Module):\n",
    "    def __init__(self, nquant, dither=False, zero_centered=True):\n",
    "        super().__init__()\n",
    "        self.nquant, self.dither, self.zero_centered = nquant, dither, zero_centered\n",
    "\n",
    "    def forward(self, out, target):\n",
    "        assert out.size(-1) == self.nquant, f'trailing dimension should hold logits {out.size()}'\n",
    "        target = discretize(target, self.nquant, dither=self.dither, zero_centered=self.zero_centered)\n",
    "        return F.nll_loss(out.view(-1, self.nquant), target.view(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cc6f31",
   "metadata": {},
   "source": [
    "## SPL Loss Functions\n",
    "\n",
    "> Loss functions adapted from the [SPL][] repository.\n",
    "\n",
    "[spl]: https://github.com/eth-ait/spl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f188b32f",
   "metadata": {},
   "source": [
    "To do:\n",
    "\n",
    "* `euler_angle_mse` is an inaccurate name, it's actually a pairwise distance\n",
    "* Geodesic needs better documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b82d316",
   "metadata": {},
   "source": [
    "In the standard evaluation on Human3.6M and AMASS the following loss functions are recommended:\n",
    "\n",
    "* Euler angle\n",
    "* Geodesic joint angle (calculated above in the VPoser loss)\n",
    "* Absolute position error (calculated above in the VPoser loss)\n",
    "* PCK: \"percentage of joints lying within a spherical threshold $\\rho$ around the target joint position\", typically reported as an AUC while varying the $\\rho$ threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f224715",
   "metadata": {},
   "source": [
    "Required packages to run tests:\n",
    "\n",
    "* [quaternion][]\n",
    "* [cv2][]\n",
    "* [gaitplotlib][]\n",
    "\n",
    "And it's necessary to have unpacked body model files with gaitplotlib as described [here][unpack].\n",
    "\n",
    "[quaternion]: https://quaternion.readthedocs.io/en/latest/\n",
    "[cv2]: https://pypi.org/project/opencv-python/\n",
    "[gaitplotlib]: https://github.com/gngdb/gaitplotlib\n",
    "[unpack]: https://github.com/gngdb/gaitplotlib#unpack-body-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df38a7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nobackup/gngdb/envs/amass/lib/python3.7/site-packages/quaternion/numba_wrapper.py:23: UserWarning: \n",
      "\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Could not import from numba, which means that some\n",
      "parts of this code may run MUCH more slowly.  You\n",
      "may wish to install numba.\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "\n",
      "  warnings.warn(warning_text)\n"
     ]
    }
   ],
   "source": [
    "# spl\n",
    "# I don't want to install tensorflow to install their whole package for testing\n",
    "!git clone https://github.com/eth-ait/spl.git 2>/dev/null\n",
    "import sys\n",
    "from pathlib import Path\n",
    "common_path = \"./spl/common\"\n",
    "if common_path not in sys.path:\n",
    "    sys.path.append(common_path)\n",
    "from conversions import (is_valid_rotmat, rotmat2euler, aa2rotmat, get_closest_rotmat, sparse_to_full, local_rot_to_global)\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86514f65",
   "metadata": {},
   "source": [
    "### SPL Implementations\n",
    "\n",
    "> Included for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8296fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spl\n",
    "\"\"\"\n",
    "SPL: training and evaluation of neural networks with a structured prediction layer.\n",
    "Copyright (C) 2019 ETH Zurich, Emre Aksan, Manuel Kaufmann\n",
    "\n",
    "This program is free software: you can redistribute it and/or modify\n",
    "it under the terms of the GNU General Public License as published by\n",
    "the Free Software Foundation, either version 3 of the License, or\n",
    "(at your option) any later version.\n",
    "\n",
    "This program is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "GNU General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU General Public License\n",
    "along with this program.  If not, see <https://www.gnu.org/licenses/>.\n",
    "\"\"\"\n",
    "def euler_diff(predictions, targets):\n",
    "    \"\"\"\n",
    "    Computes the Euler angle error as in previous work, following\n",
    "    https://github.com/una-dinosauria/human-motion-prediction/blob/master/src/translate.py#L207\n",
    "    Args:\n",
    "        predictions: np array of predicted joint angles represented as rotation matrices, i.e. in shape\n",
    "          (..., n_joints, 3, 3)\n",
    "        targets: np array of same shape as `predictions`\n",
    "    Returns:\n",
    "        The Euler angle error an np array of shape (..., )\n",
    "    \"\"\"\n",
    "    assert predictions.shape[-1] == 3 and predictions.shape[-2] == 3\n",
    "    assert targets.shape[-1] == 3 and targets.shape[-2] == 3\n",
    "    n_joints = predictions.shape[-3]\n",
    "\n",
    "    ori_shape = predictions.shape[:-3]\n",
    "    preds = np.reshape(predictions, [-1, 3, 3])\n",
    "    targs = np.reshape(targets, [-1, 3, 3])\n",
    "\n",
    "    euler_preds = rotmat2euler(preds)  # (N, 3)\n",
    "    euler_targs = rotmat2euler(targs)  # (N, 3)\n",
    "\n",
    "    # reshape to (-1, n_joints*3) to be consistent with previous work\n",
    "    euler_preds = np.reshape(euler_preds, [-1, n_joints*3])\n",
    "    euler_targs = np.reshape(euler_targs, [-1, n_joints*3])\n",
    "\n",
    "    # l2 error on euler angles\n",
    "    idx_to_use = np.where(np.std(euler_targs, 0) > 1e-4)[0]\n",
    "    euc_error = np.power(euler_targs[:, idx_to_use] - euler_preds[:, idx_to_use], 2)\n",
    "    euc_error = np.sqrt(np.sum(euc_error, axis=1))  # (-1, ...)\n",
    "\n",
    "    # reshape to original\n",
    "    return np.reshape(euc_error, ori_shape)\n",
    "\n",
    "def angle_diff(predictions, targets):\n",
    "    \"\"\"\n",
    "    Computes the angular distance between the target and predicted rotations. We define this as the angle that is\n",
    "    required to rotate one rotation into the other. This essentially computes || log(R_diff) || where R_diff is the\n",
    "    difference rotation between prediction and target.\n",
    "    Args:\n",
    "        predictions: np array of predicted joint angles represented as rotation matrices, i.e. in shape\n",
    "          (..., n_joints, 3, 3)\n",
    "        targets: np array of same shape as `predictions`\n",
    "    Returns:\n",
    "        The geodesic distance for each joint as an np array of shape (..., n_joints)\n",
    "    \"\"\"\n",
    "    assert predictions.shape[-1] == predictions.shape[-2] == 3\n",
    "    assert targets.shape[-1] == targets.shape[-2] == 3\n",
    "\n",
    "    ori_shape = predictions.shape[:-2]\n",
    "    preds = np.reshape(predictions, [-1, 3, 3])\n",
    "    targs = np.reshape(targets, [-1, 3, 3])\n",
    "\n",
    "    # compute R1 * R2.T, if prediction and target match, this will be the identity matrix\n",
    "    r = np.matmul(preds, np.transpose(targs, [0, 2, 1]))\n",
    "\n",
    "    # convert `r` to angle-axis representation and extract the angle, which is our measure of difference between\n",
    "    # the predicted and target orientations\n",
    "    angles = []\n",
    "    for i in range(r.shape[0]):\n",
    "        aa, _ = cv2.Rodrigues(r[i])\n",
    "        angles.append(np.linalg.norm(aa))\n",
    "    angles = np.array(angles)\n",
    "\n",
    "    return np.reshape(angles, ori_shape)\n",
    "\n",
    "def positional(predictions, targets):\n",
    "    \"\"\"\n",
    "    Computes the Euclidean distance between joints in 3D space.\n",
    "    Args:\n",
    "        predictions: np array of predicted 3D joint positions in format (..., n_joints, 3)\n",
    "        targets: np array of same shape as `predictions`\n",
    "    Returns:\n",
    "        The Euclidean distance for each joint as an np array of shape (..., n_joints)\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum((predictions - targets) ** 2, axis=-1))\n",
    "\n",
    "def pck(predictions, targets, thresh):\n",
    "    \"\"\"\n",
    "    Percentage of correct keypoints.\n",
    "    Args:\n",
    "        predictions: np array of predicted 3D joint positions in format (..., n_joints, 3)\n",
    "        targets: np array of same shape as `predictions`\n",
    "        thresh: radius within which a predicted joint has to lie.\n",
    "    Returns:\n",
    "        Percentage of correct keypoints at the given threshold level, stored in a np array of shape (..., len(threshs))\n",
    "    \"\"\"\n",
    "    dist = np.sqrt(np.sum((predictions - targets) ** 2, axis=-1))\n",
    "    pck = np.mean(np.array(dist <= thresh, dtype=np.float32), axis=-1)\n",
    "    return pck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac424e1",
   "metadata": {},
   "source": [
    "### Euler Angle\n",
    "\n",
    "> I don't know what euler angle convention this corresponds to, but it matches the implementation used in prior papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41ff8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "def euler_angle_mse(predictions, targets, n_joints=21):\n",
    "    \"Inputs predictions and targets are assumed to be rotation matrices\"\n",
    "    predictions = transforms.Rotation.from_matrix(predictions).as_euler()\n",
    "    targets = transforms.Rotation.from_matrix(targets).as_euler()\n",
    "    \n",
    "    predictions = predictions.view(-1, n_joints*3)\n",
    "    targets = targets.view(-1, n_joints*3)\n",
    "    \n",
    "    # l2 error on euler angles\n",
    "    #idx_to_use = np.where(np.std(euler_targs, 0) > 1e-4)[0]\n",
    "    mask = (torch.std(targets, 0) > 1e-4).float().view(1, -1)\n",
    "    euc_error = torch.square(targets*mask - predictions*mask)\n",
    "    euc_error = torch.sqrt(torch.sum(euc_error, 1))  # (-1, ...)\n",
    "    return euc_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c02358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed84a8dad9d942c4b5549e50f4528976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 21, 3, 3]) torch.Size([100, 21, 3, 3]) (100,)\n",
      "torch.Size([39, 21, 3, 3]) torch.Size([39, 21, 3, 3]) (39,)\n"
     ]
    }
   ],
   "source": [
    "# spl\n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    unpack_body_models(\"sample_data/\", tmpdirname, 1, verify=False)\n",
    "    for npz_path in npz_paths(tmpdirname):\n",
    "        cdata = np.load(npz_path)\n",
    "        poses = torch.tensor(cdata['poses'][:, 3:66]).float()\n",
    "        n, d = poses.size()\n",
    "        j = d//3\n",
    "        poses = poses.view(n, j, 3)\n",
    "        # convert poses to rotation matrices\n",
    "        rotmats = transforms.Rotation.from_rotvec(poses).as_matrix().reshape(n, j, 3, 3)\n",
    "        outputs = rotmats[:-1:6]\n",
    "        targets = rotmats[1::6]\n",
    "        loss = euler_diff(outputs.numpy(), targets.numpy())\n",
    "        _loss = euler_angle_mse(outputs.reshape(-1, 3, 3), targets.reshape(-1, 3, 3))\n",
    "        assert np.abs(loss - _loss.numpy()).max() < 1e-4\n",
    "        print(outputs.shape, targets.shape, loss.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858a0f9c",
   "metadata": {},
   "source": [
    "### Geodesic\n",
    "\n",
    "Basing this on the geodesic loss calculated in the combined Vposer loss above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7725fa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "class GeodesicLossSPL(GeodesicLossR):\n",
    "    def __init__(self, reduction='none'):\n",
    "        super().__init__(reduction=reduction)\n",
    "        \n",
    "    def bgdR(self, m1, m2):\n",
    "        assert m1.ndim == m2.ndim, \\\n",
    "            f\"Rotation matrices ndim must be equal but was {m1.ndim} {m2.ndim}\"\n",
    "        for m in [m1, m2]:\n",
    "            assert m.size(-1) == 3 and m.size(-2) == 3, \\\n",
    "                f\"Trailing 2 dimensions must 3x3 rotation matrices {m.size()}\"\n",
    "        if m1.ndim == 2:\n",
    "            # ndim 2 must be single rotation matrix\n",
    "            m1 = m1.view(1, 3, 3)\n",
    "            m2 = m2.view(1, 3, 3)\n",
    "        elif m1.ndim > 3:\n",
    "            m1 = m1.view(-1, 3, 3)\n",
    "            m2 = m2.view(-1, 3, 3)\n",
    "        batch = m1.shape[0]\n",
    "        m = torch.bmm(m1, m2.transpose(1, 2))  # batch*3*3\n",
    "\n",
    "        aa = transforms.Rotation.from_matrix(m).as_rotvec().view(batch, 3)\n",
    "        angles = torch.linalg.norm(aa, axis=1)\n",
    "        \n",
    "        return angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5cb4bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fce54fec2584a80a2caebbacfa9d8fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 21, 3, 3]) torch.Size([100, 21, 3, 3]) (100, 21)\n",
      "torch.Size([39, 21, 3, 3]) torch.Size([39, 21, 3, 3]) (39, 21)\n"
     ]
    }
   ],
   "source": [
    "# spl\n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    unpack_body_models(\"sample_data/\", tmpdirname, 1, verify=False)\n",
    "    for npz_path in npz_paths(tmpdirname):\n",
    "        cdata = np.load(npz_path)\n",
    "        poses = torch.tensor(cdata['poses'][:, 3:66])\n",
    "        n, d = poses.size()\n",
    "        j = d//3\n",
    "        poses = poses.view(n, j, 3)\n",
    "        # convert poses to rotation matrices\n",
    "        rotmats = transforms.Rotation.from_rotvec(poses).as_matrix().view(n, j, 3, 3)\n",
    "        outputs = rotmats[:-1:6]\n",
    "        targets = rotmats[1::6]\n",
    "        loss = angle_diff(outputs.numpy(), targets.numpy())\n",
    "        geodesic_loss = GeodesicLossSPL()\n",
    "        _loss = geodesic_loss(outputs.reshape(-1,3,3), targets.reshape(-1,3,3))\n",
    "        assert np.allclose(loss, _loss.numpy().reshape(*loss.shape))\n",
    "        print(outputs.shape, targets.shape, loss.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66807d5e",
   "metadata": {},
   "source": [
    "### Positional\n",
    "\n",
    "Basing this on the VPoser loss above, has to contain a body model to do forward kinematics to estimate the joint positions given a fixed body model. (Although, I think betas are often provided for AMASS examples, so it may be more correct to use those.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013cf086",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "class PositionalLossSPL(ForwardKinematicLoss):\n",
    "    def forward(\n",
    "        self,\n",
    "        aa_out=None,\n",
    "        pose_target=None,\n",
    "        bm_orig=None,\n",
    "        bm_rec=None,\n",
    "        positions=None,\n",
    "        target_positions=None\n",
    "    ):\n",
    "        for p in [positions, target_positions]:\n",
    "            if p is not None:\n",
    "                assert p.ndim == 3\n",
    "                assert p.size(-1) == 3, \"final dim must contain 3D locations\"\n",
    "        if pose_target is not None:\n",
    "            if pose_target.ndim == 3:\n",
    "                bs, f, d = pose_target.size()\n",
    "                n = bs*f\n",
    "                assert d == n_joints*3\n",
    "            elif pose_target.ndim == 2:\n",
    "                n, d = pose_target.size()\n",
    "\n",
    "        # forward kinematics\n",
    "        no_bm_output = bm_orig is None or bm_rec is None\n",
    "        no_positions = positions is None or target_positions is None\n",
    "        if no_bm_output and no_positions:\n",
    "            bm_orig, bm_rec = self.kinematics(aa_out.reshape(n, d), pose_target.reshape(n, d))\n",
    "            positions = bm_rec.Jtr\n",
    "            target_positions = bm_orig.Jtr\n",
    "            \n",
    "        return torch.sqrt(torch.square(positions - target_positions).sum(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa040778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b79486d801642eba9e3bed473a6318d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 52, 3]) torch.Size([100, 52, 3]) (100, 52)\n",
      "torch.Size([39, 52, 3]) torch.Size([39, 52, 3]) (39, 52)\n"
     ]
    }
   ],
   "source": [
    "# spl\n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    unpack_body_models(\"sample_data/\", tmpdirname, 1, verify=False)\n",
    "    for npz_path in npz_paths(tmpdirname):\n",
    "        cdata = np.load(npz_path)\n",
    "        _poses = torch.tensor(cdata['poses'][::6, 3:66], dtype=torch.float, requires_grad=True)\n",
    "        n, d = _poses.size()\n",
    "        j = d//3\n",
    "        poses = _poses.view(n, j, 3)\n",
    "        poses = poses.view(-1, j*3)\n",
    "        # convert poses to rotation matrices\n",
    "        with torch.no_grad():\n",
    "            positions = body_model(pose_body=poses).Jtr\n",
    "        outputs = positions[:-1]\n",
    "        targets = positions[1:]\n",
    "        loss = positional(outputs.numpy(), targets.numpy())\n",
    "        positional_loss_spl = PositionalLossSPL(body_model)\n",
    "        _loss = positional_loss_spl(poses[:-1], poses[1:])\n",
    "        assert np.allclose(loss, _loss.detach().numpy())\n",
    "        _loss.mean().backward()\n",
    "        assert not torch.any(torch.isnan(_poses.grad)), \"Gradient contains NaNs\"\n",
    "        assert torch.abs(_poses.grad).max() > 1e-6, \"gradient is zero\"\n",
    "        print(outputs.shape, targets.shape, loss.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22b0b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0353765bbbc42afa3fabd2c931901fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 24, 3]) torch.Size([100, 24, 3]) (100, 24)\n",
      "torch.Size([39, 24, 3]) torch.Size([39, 24, 3]) (39, 24)\n"
     ]
    }
   ],
   "source": [
    "# spl\n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    unpack_body_models(\"sample_data/\", tmpdirname, 1, verify=False)\n",
    "    for npz_path in npz_paths(tmpdirname):\n",
    "        cdata = np.load(npz_path)\n",
    "        _poses = torch.tensor(cdata['poses'][::6, :3*24], dtype=torch.float, requires_grad=True)\n",
    "        n, d = _poses.size()\n",
    "        j = d//3\n",
    "        poses = _poses.view(n, j, 3)\n",
    "        poses = poses.view(-1, j*3)\n",
    "        # convert poses to rotation matrices\n",
    "        positions = fk_engine.from_aa(poses)\n",
    "        outputs = positions[:-1]\n",
    "        targets = positions[1:]\n",
    "        loss = positional(outputs.detach().numpy(), targets.detach().numpy())\n",
    "        positional_loss_spl = PositionalLossSPL(None)\n",
    "        _loss = positional_loss_spl(positions=outputs, target_positions=targets)\n",
    "        assert np.allclose(loss, _loss.detach().numpy())\n",
    "        _loss.mean().backward()\n",
    "        assert not torch.any(torch.isnan(_poses.grad)), \"Gradient contains NaNs\"\n",
    "        assert torch.abs(_poses.grad).max() > 1e-6, \"gradient is zero\"\n",
    "        print(outputs.shape, targets.shape, loss.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988a0236",
   "metadata": {},
   "source": [
    "### PCK\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c3d259",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "class PCK_SPL(ForwardKinematicLoss):\n",
    "    def forward(\n",
    "        self,\n",
    "        aa_out=None,\n",
    "        pose_target=None,\n",
    "        positions=None,\n",
    "        target_positions=None,\n",
    "        thresh=None,\n",
    "        bm_orig=None,\n",
    "        bm_rec=None,\n",
    "        n_joints=21\n",
    "    ):\n",
    "        assert thresh is not None\n",
    "        for p in [positions, target_positions]:\n",
    "            if p is not None:\n",
    "                assert p.ndim == 3\n",
    "                assert p.size(-1) == 3, \"final dim must contain 3D locations\"\n",
    "        if pose_target is not None:\n",
    "            if pose_target.ndim == 3:\n",
    "                bs, f, d = pose_target.size()\n",
    "                n = bs*f\n",
    "                assert d == n_joints*3\n",
    "            elif pose_target.ndim == 2:\n",
    "                n, d = pose_target.size()\n",
    "\n",
    "        # forward kinematics\n",
    "        no_bm_output = bm_orig is None or bm_rec is None\n",
    "        no_positions = positions is None or target_positions is None\n",
    "        if no_bm_output and no_positions:\n",
    "            bm_orig, bm_rec = self.kinematics(aa_out.reshape(n, d), pose_target.reshape(n, d))\n",
    "            positions = bm_rec.Jtr\n",
    "            target_positions = bm_orig.Jtr\n",
    "            \n",
    "        # percentage of coordinates in the ball defined by thresh around a joint\n",
    "        n, d, _ = positions.size()\n",
    "        dist = torch.sqrt(torch.square(positions - target_positions).sum(2))\n",
    "        return torch.mean((dist <= thresh).float(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7b6252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d8895f8a174de8aa370ea1d6ae5a22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# spl\n",
    "pck_thresholds = [0.001, 0.005, 0.01, 0.02, 0.05, 0.1, 0.15, 0.2, 0.3]\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    unpack_body_models(\"sample_data/\", tmpdirname, 1, verify=False)\n",
    "    for npz_path in npz_paths(tmpdirname):\n",
    "        cdata = np.load(npz_path)\n",
    "        poses = torch.tensor(cdata['poses'][:, 3:66]).float()\n",
    "        n, d = poses.size()\n",
    "        j = d//3\n",
    "        poses = poses.view(n, j, 3)\n",
    "        poses = poses[::6].view(-1, j*3)\n",
    "        # convert poses to rotation matrices\n",
    "        with torch.no_grad():\n",
    "            positions = body_model(pose_body=poses).Jtr\n",
    "        outputs = positions[:-1]\n",
    "        targets = positions[1:]\n",
    "        _pck = PCK_SPL(body_model)\n",
    "        for thresh in pck_thresholds:\n",
    "            loss = pck(outputs.numpy(), targets.numpy(), thresh)\n",
    "            _loss = _pck(poses[:-1], poses[1:], thresh=thresh)\n",
    "            assert np.allclose(loss, _loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1216320",
   "metadata": {},
   "source": [
    "## PCK AUC\n",
    "\n",
    "> Area under curve for PCK losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de79f3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\"\"\"\n",
    "SPL: training and evaluation of neural networks with a structured prediction layer.\n",
    "Copyright (C) 2019 ETH Zurich, Emre Aksan, Manuel Kaufmann\n",
    "\n",
    "This program is free software: you can redistribute it and/or modify\n",
    "it under the terms of the GNU General Public License as published by\n",
    "the Free Software Foundation, either version 3 of the License, or\n",
    "(at your option) any later version.\n",
    "\n",
    "This program is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "GNU General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU General Public License\n",
    "along with this program.  If not, see <https://www.gnu.org/licenses/>.\n",
    "\"\"\"\n",
    "\n",
    "def calculate_auc(pck_values, pck_thresholds, target_length):\n",
    "    \"\"\"Calculate area under a curve (AUC) metric for PCK.\n",
    "\n",
    "    If the sequence length is shorter, we ignore some of the high-tolerance PCK values in order to have less\n",
    "    saturated AUC.\n",
    "    Args:\n",
    "        pck_values (list): PCK values.\n",
    "        pck_thresholds (list): PCK threshold values.\n",
    "        target_length (int): determines for which time-step we calculate AUC.\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    # Due to the saturation effect, we consider a limited number of PCK thresholds in AUC calculation.\n",
    "    if target_length < 6:\n",
    "        n_pck = 6\n",
    "    elif target_length < 12:\n",
    "        n_pck = 7\n",
    "    elif target_length < 18:\n",
    "        n_pck = 8\n",
    "    else:\n",
    "        n_pck = len(pck_thresholds)\n",
    "\n",
    "    norm_factor = np.diff(pck_thresholds[:n_pck]).sum()\n",
    "    auc_values = []\n",
    "    for i in range(n_pck - 1):\n",
    "        auc = (pck_values[i] + pck_values[i + 1]) / 2 * (pck_thresholds[i + 1] - pck_thresholds[i])\n",
    "        auc_values.append(auc)\n",
    "    return np.array(auc_values).sum() / norm_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b2bfb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dd7ee9ae4ba4cd4b64fe9bd45f9b928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9904558266569304\n",
      "AUC: 0.5497117626587016\n"
     ]
    }
   ],
   "source": [
    "# spl\n",
    "\n",
    "pck_thresholds = [0.001, 0.005, 0.01, 0.02, 0.05, 0.1, 0.15, 0.2, 0.3]\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    unpack_body_models(\"sample_data/\", tmpdirname, 1, verify=False)\n",
    "    for npz_path in npz_paths(tmpdirname):\n",
    "        cdata = np.load(npz_path)\n",
    "        _poses = torch.tensor(cdata['poses'][::6, :3*24], dtype=torch.float, requires_grad=True)\n",
    "        n, d = _poses.size()\n",
    "        j = d//3\n",
    "        poses = _poses.view(n, j, 3)\n",
    "        poses = poses.view(-1, j*3)\n",
    "        # convert poses to rotation matrices\n",
    "        positions = fk_engine.from_aa(poses)\n",
    "        outputs = positions[:-1]\n",
    "        targets = positions[1:]\n",
    "        _pck = PCK_SPL(None)\n",
    "        \n",
    "        pck_vals = []\n",
    "        for thresh in pck_thresholds:\n",
    "            loss = pck(outputs.detach().numpy(), targets.detach().numpy(), thresh)\n",
    "            pck_vals.append(loss)\n",
    "            _loss = _pck(positions=outputs, target_positions=targets, thresh=thresh)\n",
    "            assert np.allclose(loss, _loss)\n",
    "        pck_vals = [np.mean(v) for v in pck_vals]\n",
    "        print(f\"AUC: {calculate_auc(pck_vals, pck_thresholds, 6)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5611f259",
   "metadata": {},
   "source": [
    "## Logging Utilities\n",
    "\n",
    "> Utility functions for logging losses during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103e0c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def all_univariate_tensors_in(d):\n",
    "    \"Utility function for logging with a callback function\"\n",
    "    def is_univariate_tensor(x):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            return x.nelement() == 1\n",
    "\n",
    "    return {k: v for k, v in d.items() if is_univariate_tensor(v)}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
