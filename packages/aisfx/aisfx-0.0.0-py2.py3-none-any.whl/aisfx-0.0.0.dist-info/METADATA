Metadata-Version: 2.1
Name: aisfx
Version: 0.0.0
Summary: Representation Learning for the Automatic Indexing of Sound Effects Libraries (ISMIR 2022): Deep audio embeddings pre-trained on UCS & Non-UCS-compliant datasets.
Project-URL: Source, https://github.com/alisonbma/aisfx
Project-URL: Tracker, https://github.com/alisonbma/aisfx/issues
Author-email: Alison Bernice Ma <ama67@gatech.edu>, Alexander Lerch <alexander.lerch@gatech.edu>
Keywords: audio,deep,embeddings,learning,machine,pytorch,representation,sound effects library,universal category system
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: MIT License
Classifier: Topic :: Multimedia :: Sound/Audio :: Analysis
Requires-Dist: argparse==1.1
Requires-Dist: h5py==3.1.0
Requires-Dist: librosa==0.9.2
Requires-Dist: matplotlib==3.3.4
Requires-Dist: numpy==1.19.5
Requires-Dist: openpyxl==3.0.9
Requires-Dist: pandas==1.1.3
Requires-Dist: pytorch-metric-learning==1.0.0
Requires-Dist: pyyaml==5.3.1
Requires-Dist: ray==1.7.0
Requires-Dist: ray[rllib]
Requires-Dist: scikit-learn==0.24.2
Requires-Dist: scipy==1.5.4
Requires-Dist: seaborn==0.11.2
Requires-Dist: tensorboard==2.7.0
Requires-Dist: tqdm==4.62.3
Requires-Dist: xlrd
Description-Content-Type: text/markdown

# aiSFX
 Representation Learning for the Automatic Indexing of Sound Effects Libraries (ISMIR 2022): Deep audio embeddings pre-trained on UCS & Non-UCS-compliant datasets.
 
This work was inspired by the creation of the [Universal Category System (UCS)](https://universalcategorysystem.com), an industry-proposed public domain initiative initialized by [Tim Nielsen](https://www.imdb.com/name/nm0631004), [Justin Drury](https://twitter.com/jaydee2190), [Kai Paquin](https://www.imdb.com/name/nm5226068), and others. First launching in the fall of 2020, UCS offers a standardized framework for sound effects library metadata designed by and for sound designers and editors.

# Acknowledgements
We would like to thank those who took the time to provide the data required to conduct this research as well as those who took the time to share their insights and software licenses for tools regarding sound search, query, and retrieval.

[Universal Category System (UCS)](https://universalcategorysystem.com) • [Alex Lane](https://www.alex-lane.com) • [All You Can Eat Audio](https://allyoucaneataudio.com) • [Articulated Sounds](https://articulatedsounds.com) • [Audio Shade](https://audioshade.com) • [aXLSound](https://axlsound.com) • [Big Sound Bank](https://bigsoundbank.com) • [BaseHead](https://baseheadinc.com) • [Bonson](https://www.bonson.ca) • [BOOM Library](https://www.boomlibrary.com) • [Frick & Traa](https://www.frickandtraa.com) • [Hzandbits](https://www.hzandbits.com) • [InspectorJ](https://www.jshaw.co.uk/inspectorj) • [Kai Paquin](https://www.imdb.com/name/nm5226068) • [KEDR Audio](https://www.asoundeffect.com/sounddesigner/kedr-audio) • [Krotos Audio](https://www.krotosaudio.com) • [Nikola Simikic](https://www.imdb.com/name/nm4851270) • [Penguin Grenade](https://www.paulstoughton.com) • [Pro Sound Effects](https://www.prosoundeffects.com) • [Rick Allen Creative](https://www.rickallencreative.com) • [Sononym](https://www.sononym.net/) • [Sound Ideas](https://www.sound-ideas.com) • [Soundly](https://www.getsoundly.com) • [Soundminer](https://store.soundminer.com) • [Storyblocks](https://www.storyblocks.com) • [Tim Nielsen](https://www.imdb.com/name/nm0631004) • [Thomas Rex Beverly](https://thomasrexbeverly.com) • [ZapSplat](https://www.zapsplat.com)

# Citations

Please cite the paper below if you use it in your work.

This paper has been accepted at the 23rd International Society for Music Information Retrieval Conference (ISMIR) in Bengaluru, India (December 04-08, 2022). To cite a pre-print, please refer to the following.

[1] [Representation Learning for the Automatic Indexing of Sound Effects Libraries](https://www.xxxxxx)

      @article{rlaiSFX2022},
            title = {Representation Learning for the Automatic Indexing of Sound Effects Libraries},
            author = {Ma, Alison Bernice and Lerch, Alexander},
            journal={arXiv preprint arXiv:xxxx.xxxxx},
            year = {2022},
            archivePrefix = {arXiv},
      }

# Pre-trained Model & Paper License

This pre-trained model and paper [1] is made available under a Creative Commons Attribution 4.0 International License (CC BY 4.0).
