# @package _global_

optimizer:
  _target_: torch.optim.Adam
  lr: 0.0004

model:
  _target_: slot_attention_pytorch.model.SlotAttentionAE
  encoder:
    _target_: torch.nn.Sequential
    _args_:
      - _target_: torch.nn.Conv2d
        in_channels: ${model.input_channels}
        out_channels: 32
        kernel_size: 5
        padding: 2
      - _target_: torch.nn.ReLU
      - _target_: torch.nn.Conv2d
        in_channels: 32
        out_channels: 32
        kernel_size: 5
        padding: 2
      - _target_: torch.nn.ReLU
      - _target_: torch.nn.Conv2d
        in_channels: 32
        out_channels: 32
        kernel_size: 5
        padding: 2
      - _target_: torch.nn.ReLU
      - _target_: torch.nn.Conv2d
        in_channels: 32
        out_channels: 32
        kernel_size: 5
        padding: 2
      - _target_: torch.nn.ReLU
      - _target_: slot_attention_pytorch.positional_embedding.PositionalEmbedding
        width: ${dataset.width}
        height: ${dataset.height}
        channels: 32
      - _target_: torch.nn.Flatten
        start_dim: 2
        end_dim: 3
      - _target_: torch.nn.GroupNorm
        num_groups: 1
        num_channels: 32
        affine: true
        eps: 0.001
      - _target_: torch.nn.Conv2d
        in_channels: 32
        out_channels: 32
        kernel_size: 1
      - _target_: torch.nn.ReLU
      - _target_: torch.nn.Conv2d
        in_channels: 32
        out_channels: 32
        kernel_size: 1
  decoder:
    _target_: torch.nn.Sequential
    _args_:
      - _target_: slot_attention_pytorch.positional_embedding.PositionalEmbedding
        width: ${dataset.width}
        height: ${dataset.height}
        channels: ${model.slot_attention_module.latent_size}
      - _target_: torch.nn.Conv2d
        in_channels: ${model.slot_attention_module.latent_size}
        out_channels: 32
        kernel_size: 5
        padding: 2
      - _target_: torch.nn.ReLU
      - _target_: torch.nn.Conv2d
        in_channels: 32
        out_channels: 32
        kernel_size: 5
        padding: 2
      - _target_: torch.nn.ReLU
      - _target_: torch.nn.Conv2d
        in_channels: 32
        out_channels: 32
        kernel_size: 5
        padding: 2
      - _target_: torch.nn.ReLU
      - _target_: torch.nn.Conv2d
        in_channels: 32
        out_channels: 4
        kernel_size: 3
        padding: 1
  slot_attention_module:
    _target_: slot_attention_pytorch.slot_attention_module.SlotAttentionModule
    num_slots: ${dataset.max_num_objects}
    channels_enc: 32
    latent_size: 64
    attention_iters: 3
    eps: 1e-8
    mlp_size: 128
  w_broadcast: ${dataset.width}
  h_broadcast: ${dataset.height}
  width: ${dataset.width}
  height: ${dataset.height}
  input_channels: 3