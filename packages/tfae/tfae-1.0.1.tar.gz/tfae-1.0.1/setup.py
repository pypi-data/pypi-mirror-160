# -*- coding: utf-8 -*-
from setuptools import setup

packages = \
['tfae',
 'tfae.bottlenecks',
 'tfae.models',
 'tfae.regularizers',
 'tfae.schedulers']

package_data = \
{'': ['*']}

install_requires = \
['tensorflow>=2.9.1,<3.0.0']

setup_kwargs = {
    'name': 'tfae',
    'version': '1.0.1',
    'description': "TFAE it's a TensorFlow extension for building and training different types of autoencoders",
    'long_description': '# TensorFlow Autoencoders (TFAE)\nThis package is a [TensorFlow](https://tensorflow.org/) extension for building and training different types of autoencoders.\n\n## Table of contents\n- [Motivation]()\n- [Current package]()\n- [Installation]()\n- [Usage]()\n- [API reference]()\n- [Contribution]()\n\n## Motivation\n\nAutoencoders – AE – are widely used in various tasks: from computer vision to recommender systems and many others.\n\nTensorflow, being a flexible tool, does not provide AE-specific tools though.\n\nThe lack of such tools results into redundant code. Even [official tutorial](https://www.tensorflow.org/tutorials/generative/cvae) on this topic includes explicit looping through training steps and manual computation of losses and gradients.\n\nCode like this is difficult to maintain.\n\nTFAE makes process of building and training AEs easy yet flexible, keeping traditional TensorFlow style.\n\n## Current package\n\nTFAE is a set of extensions for standard Tensorflow classes, such as Layers and Models. These can be used to build a wide range of models: shallow and deep, classical and variational.\n\nIn addition to layers and models, it contains extensions for Regularizers and Callbacks, which are used to control the training process, such as [β-VAE](https://openreview.net/pdf?id=Sy2fzU9gl), [Cyclical β-annealing](https://arxiv.org/pdf/1903.10145.pdf) and others.\n\n## Installation\n\nTFAE can be installed directly from [PyPI](https://pypi.org/project/tfae/):\n\n```console\npip install tfae\n```\nTensorFlow is the only dependency. Python 3.8-3.10 is required.\n\n## Usage\n\nLet\'s take a quick example.\n\nHere we build and train shallow variational autoencoder regularized with KL-divergence:\n\n```python\nimport tensorflow as tf\n\nfrom tfae.models import Autoencoder\nfrom tfae.bottlenecks import GaussianBottleneck\nfrom tfae.regularizers import GaussianKLDRegularizer\n\nmodel = Autoencoder(\n    bottleneck=GaussianBottleneck(\n        latent_dim=32,\n        kernel_regularizer=GaussianKLDRegularizer(),\n    ),\n)\n\nmodel.compile(...)\n\nmodel.fit(...)\n```\n\nNow we can use this model to encode data:\n```python\nencoded = model.encoder.predict(...)\n```\n\nOr generate new samples:\n```python\ngenerated = model.decoder.predict(...)\n```\n\nOf course, it\'s possible to build deeper models and sophisticate training process.\n\nCheck out more examples:\n\n- [Search similar movies using autoencoder](https://colab.research.google.com/drive/1RPX2j1q8EeMBc-QZBA667EiBsXSR34Bp?usp=sharing)\n- [Generating handwritten digits with Convolutional Variational Autoencoder](https://colab.research.google.com/drive/1Lt1haqMNomDL8B1KZKDe9sHZu6y4eUmG?usp=sharing)\n\nor explore API in the following section.\n\n## API reference\n\nTFAE includes:\n- [bottlenecks](#bottlenecks)\n- [models](#models)\n- [regularizers](#regularizers)\n- [schedulers](#schedulers)\n\n### Bottlenecks\n\nBottlenecks are layers placed in the middle of a model, connecting encoder with decoder.\n\nEvery bottleneck extends `BaseBottleneck`, which in turn extends `tf.keras.layers.Layer`.\n\n#### Vanilla bottleneck\n\n`VanillaBottleneck` it\'s a "semantic stub" for a dense layer – the most simple autoencoder bottleneck.\n\n#### Variational bottlenecks\n\n`VariationalBottleneck` is a subclass for building model implementing variational inference. TFAE includes two bottlenecks for variational inference: Gaussian and Bernoulli:\n\n```mermaid\nclassDiagram\n    `tf.keras.layers.Layer` <|-- BaseBottleneck\n    BaseBottleneck <|-- VanillaBottleneck\n    BaseBottleneck <|-- VariationalBottleneck\n    VariationalBottleneck <|-- GaussianBottleneck\n    VariationalBottleneck <|-- BernoulliBottleneck\n\n    BaseBottleneck : int latent_dim\n    VariationalBottleneck : int parameters\n```\n\n### Models\n\nTFAE includes two subclasses of `tf.keras.Model`: `Autoencoder` and `DeepAutoencoder`.\n\n```mermaid\nclassDiagram\n    `tf.keras.Model` <|-- Autoencoder\n    Autoencoder <|-- DeepAutoencoder\n\n    Autoencoder : BaseBottleneck bottleneck\n    Autoencoder : make_encoder()\n    Autoencoder : make_decoder()\n    DeepAutoencoder : Callable add_hidden\n```\n\n#### Autoencoder\n\n`Autoencoder` represents a simplest form of autoencoder with only one hidden layer as a bottleneck. [See usage example](https://colab.research.google.com/drive/1RPX2j1q8EeMBc-QZBA667EiBsXSR34Bp?usp=sharing).\n\n#### DeepAutoencoder\n\n`DeepAutoencoder` extends `Autoencoder` and allows to build deeper models in a functional way: it\'s `add_hidden` method constructs additional hidden layers.\n\nLet\'s take a quick example how `add_hidden` works.\n\nIt takes four parameters:\n- input layer\n- number of current layer\n- shape of the input layer\n- dimensionality of the bottleneck\n\nAnd returns a tuple of a new layer and a boolean indicating that current layer is the last.\n\nThis method is applied to both, encoder and decoder (but for decoder in a "mirror manner").\n\nThe following example demostrates how to create encoder and decoder with two hidden layers each. And both have a pyramidal structure:\n\n```python\nfrom tfae.models import DeepAutoencoder\n\ndef add_hidden(\n    x: tf.keras.layers.Layer,\n    layer_num: int,\n    input_shape: tf.TensorShape,\n    latent_dim: int,\n) -> tuple[tf.keras.layers.Layer, bool]:\n\n    number_of_hidden_layers = 2\n\n    divisor = (latent_dim / input_shape[-1]) ** (layer_num / (number_of_hidden_layers + 1))\n    units = int(divisor * input_shape[-1])\n\n    x = tf.keras.layers.Dense(units)(x)\n\n    return x, layer_num == number_of_hidden_layers\n\nmodel = DeepAutoencoder(\n    bottleneck=...\n    add_hidden=add_hidden,\n)\n```\n\n#### Custom models\n\nCustom models can be made by extending `Autoencoder` class. [See an example](https://colab.research.google.com/drive/1Lt1haqMNomDL8B1KZKDe9sHZu6y4eUmG?usp=sharing).\n\n### Regularizers\n\nIt often proves useful to regularize bottleneck, so encoder could learn better and disentangled representation.\n\nTFAE includes:\n- `L2Regularizer` for `VanillaBottleneck`\n- `GaussianKLDRegularizer` and `GaussianReversedKLDRegularizer` for `GaussianBottleneck`\n\nEvery TFAE regularizer extends `BaseRegularizer`, which contains property `beta: float` – regularization factor:\n\n```mermaid\nclassDiagram\n    `tf.keras.regularizers.Regularizer` <|-- BaseRegularizer\n    BaseRegularizer <|-- L2Regularizer\n    BaseRegularizer <|-- GaussianKLDRegularizer\n    BaseRegularizer <|-- GaussianReversedKLDRegularizer\n\n    BaseRegularizer: float beta\n```\n\nA custom regulirizer can be applied by extending `BaseRegularizer`.\n\n### Schedulers\n\nRecent papers has shown that constant regularization factor can be an obstacle on the way to the better latent representation:\n\n- [β-VAE](https://openreview.net/pdf?id=Sy2fzU9gl)\n- [Fixing Broken ELBO](https://proceedings.mlr.press/v80/alemi18a/alemi18a.pdf)\n- [Cyclical Annealing Schedule](https://arxiv.org/pdf/1903.10145.pdf)\n- [β-annealed VAE](https://ml4physicalsciences.github.io/2020/files/NeurIPS_ML4PS_2020_133.pdf)\n\nAll these papers are suggesting to vary regularization factor – let\'s call it `β` – over time.\n\nTFAE contains `DASRScheduler` which can handle different schedules covering the aforementioned papers.\n\nEvery scheduler extends `BaseScheduler` which extends `tf.keras.callbacks.Callback`:\n\n```mermaid\nclassDiagram\n    `tf.keras.callbacks.Callback` <|-- BaseScheduler\n    BaseScheduler <|-- DASRScheduler\n\n    BaseScheduler: calc()\n\n    DASRScheduler: float start_value\n    DASRScheduler: float end_value\n    DASRScheduler: int delay\n    DASRScheduler: int attack\n    DASRScheduler: int sustain\n    DASRScheduler: int release\n    DASRScheduler: int cycles\n```\n\n#### DASRScheduler\n\n"DASR" stands for Delay, Attack, Sustain, Release.\n\nLet\'s say in "phase 1" we want to keep `β=0` for first 5 epochs, then gradually rise it up to `β=1` for 10 more epochs. In "phase 2" we\'ll keep `β=1` until loss stops improving:\n\n```python\nfrom tfae.models import Autoencoder\nfrom tfae.bottlenecks import GaussianBottleneck\nfrom tfae.regularizers import GaussianKLDRegularizer\nfrom tfae.schedulers import DASRScheduler\n\n# Creating scheduler which will keep β=0 for 5 epochs\n# and then gradually raise it up to β=1 for 10 more epochs:\nscheduler = DASRScheduler(\n    start_value=0.0,\n    end_value=1.0,\n    delay=5,\n    attack=10,\n)\n\n# Note how we pass scheduler.value to the regularizer:\nmodel = Autoencoder(\n    bottleneck=GaussianBottleneck(\n        latent_dim=32,\n        kernel_regulirizer=GaussianKLDRegularizer(\n            beta=scheduler.value,\n        )\n    )\n)\n\nmodel.compile(...)\n\n# Phase 1.\n#\n# Scheduler have auto-calculated attribute "duration"\n# which tells how many epochs it takes\n# to go through all scheduled values of β.\n# \n# We also pass scheduler as a callback\n# so so that he can be updated:\nmodel.fit(\n    ...\n    epochs=scheduler.duration,\n    callbacks=[\n        scheduler,\n    ],\n)\n\n# Phase 2.\n#\n# Here we continue training until loss stops improving:\nmodel.fit(\n    ...,\n    initial_epoch=scheduler.duration,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(...),\n    ]\n)\n```\n\nLet\'s take one more example and implement schedule for [cyclical β-annealing](https://arxiv.org/pdf/1903.10145.pdf):\n\n```python\nscheduler = DASRScheduler(\n    start_value=0.0,\n    end_value=1.0,\n    attack=10,\n    sustain=10,\n    cycles=4,\n)\n```',
    'author': 'Artem Legotin',
    'author_email': 'hello@artemlegotin.com',
    'maintainer': None,
    'maintainer_email': None,
    'url': 'https://github.com/arlegotin/TFAE',
    'packages': packages,
    'package_data': package_data,
    'install_requires': install_requires,
    'python_requires': '>=3.7,<=3.10',
}


setup(**setup_kwargs)
